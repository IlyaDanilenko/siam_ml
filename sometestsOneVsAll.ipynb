{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "frame_test = pd.read_csv(\"hq_markup_train.csv\")\n",
    "frame = pd.read_csv(\"markup_train.csv\")\n",
    "\n",
    "def get_sample(index, frame):\n",
    "    uuid = frame.iloc[index, 0]\n",
    "    with open(f'data/{uuid}', 'r') as f:\n",
    "        content = f.read()\n",
    "    rows = [line.split('\\t') for line in content.strip().split('\\n')]\n",
    "    data = pd.DataFrame(rows, columns=['time', 'delta_p', 'p_'], dtype=float)\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_empty_indexes_2(data):\n",
    "    \"\"\"\n",
    "    Поиск id файлов из строк датафрейма для их исключения на этапе удаления данных из датафрейма\n",
    "    \"\"\"\n",
    "    B=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        try:\n",
    "            df=get_sample(i,data)\n",
    "        except ValueError:\n",
    "            B.append(data.iloc[i,0])\n",
    "    return B\n",
    "\n",
    "\n",
    "class SiamDataset(Dataset):\n",
    "    def __init__(self, siam_dataset_describe:pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self.siam_dataset_describe = siam_dataset_describe\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.siam_dataset_describe.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = get_sample(idx, self.siam_dataset_describe) #.to_numpy(dtype=np.float64)\n",
    "        # t = x[\"time\"].to_numpy(dtype=np.float64)\n",
    "        x = x[[\"delta_p\", \"p_\"]].to_numpy(dtype=np.float64)\n",
    "        \n",
    "\n",
    "\n",
    "        # 7) Возвращаем (X, Y)\n",
    "        return x, self.siam_dataset_describe.iloc[idx][['Некачественное ГДИС', 'Влияние ствола скважины', 'Радиальный режим', 'Линейный режим', 'Билинейный режим', 'Сферический режим', 'Граница постоянного давления', 'Граница непроницаемый разлом']].to_numpy(dtype=np.int8)\n",
    "\n",
    "# B=find_empty_indexes_2(frame)#Поиск id с отсутствующими файлами для исключения\n",
    "# frame=frame[~frame['file_name'].isin(B)]#Фильтрация данных\n",
    "# frame = frame[~frame[\"file_name\"].isin(frame_test[\"file_name\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_varlen(batch):\n",
    "    \"\"\"\n",
    "    batch: список [(x_i, y_i), (x_j, y_j), ...].\n",
    "    Выравниваем каждый x_i внутри батча до max_len батча,\n",
    "    возвращаем (x_padded, lengths, y_tensor).\n",
    "    \"\"\"\n",
    "    max_len = max(x.shape[0] for x, _ in batch)\n",
    "    batch_size = len(batch)\n",
    "    input_dim = batch[0][0].shape[1]\n",
    "    num_classes = batch[0][1].shape[0]\n",
    "    \n",
    "    x_padded = torch.zeros(batch_size, max_len, input_dim, dtype=torch.float32)\n",
    "    lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    y_tensor = torch.zeros(batch_size, num_classes, dtype=torch.float32)\n",
    "    \n",
    "    for i, (x_i, y_i) in enumerate(batch):\n",
    "        T_i = x_i.shape[0]\n",
    "        x_padded[i, :T_i, :] = torch.from_numpy(x_i)\n",
    "        lengths[i] = T_i\n",
    "        y_tensor[i] = torch.from_numpy(y_i)\n",
    "    \n",
    "    return x_padded, lengths, y_tensor\n",
    "\n",
    "class LSTMSingleClassModel(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-модель для бинарной классификации (один выход).\n",
    "    Использует pack_padded_sequence для эффективной обработки переменной длины.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, bidirectional=False, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        dir_factor = 2 if bidirectional else 1\n",
    "        self.classifier = nn.Linear(hidden_dim * dir_factor, 1)  # один выход!\n",
    "    \n",
    "    def forward(self, x_padded, lengths):\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_padded, \n",
    "            lengths.cpu(),\n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        \n",
    "        # Выделяем финальное скрытое состояние\n",
    "        if self.lstm.bidirectional:\n",
    "            # h_n shape: [num_layers*2, batch_size, hidden_dim]\n",
    "            # Возьмём последний слой h_n[-2], h_n[-1] и склеим\n",
    "            num_layers = self.lstm.num_layers\n",
    "            hidden_dim = self.lstm.hidden_size\n",
    "            batch_size = x_padded.size(0)\n",
    "            \n",
    "            # Развёртываем: [num_layers, 2, batch_size, hidden_dim]\n",
    "            h_last_layer = h_n.view(num_layers, 2, batch_size, hidden_dim)\n",
    "            h_forward = h_last_layer[-1, 0, :, :]   # [batch_size, hidden_dim]\n",
    "            h_backward = h_last_layer[-1, 1, :, :]  # [batch_size, hidden_dim]\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)  # [batch_size, 2*hidden_dim]\n",
    "        else:\n",
    "            # h_n[-1] => [batch_size, hidden_dim]\n",
    "            h_final = h_n[-1]\n",
    "        \n",
    "        logits = self.classifier(h_final)  # [batch_size, 1]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_class(\n",
    "    model, \n",
    "    data_loader, \n",
    "    device, \n",
    "    lr=1e-3, \n",
    "    num_epochs=5, \n",
    "    class_index=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Обучает model (LSTMSingleClassModel) предсказывать класс с индексом class_index.\n",
    "    data_loader: выдаёт (x_padded, lengths, y_full), где y_full имеет shape [batch_size, num_classes].\n",
    "                 мы берём только y_full[:, class_index].\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x_padded, lengths, y_full in data_loader:\n",
    "            x_padded = x_padded.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Вытаскиваем бинарную метку для нужного класса c:\n",
    "            y_target = y_full[:, class_index].to(device)  # shape [batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_padded, lengths).squeeze(-1)  # shape [batch_size, 1] => [batch_size]\n",
    "            \n",
    "            loss = criterion(logits, y_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f\"[Class {class_index}] Epoch {epoch+1}/{num_epochs}, Loss={avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Обучаем модель для класса 0 (one-vs-all) ===\n",
      "[Class 0] Epoch 1/10, Loss=0.3478\n",
      "[Class 0] Epoch 2/10, Loss=0.3244\n",
      "[Class 0] Epoch 3/10, Loss=0.3108\n",
      "[Class 0] Epoch 4/10, Loss=0.2978\n",
      "[Class 0] Epoch 5/10, Loss=0.2839\n",
      "[Class 0] Epoch 6/10, Loss=0.2805\n",
      "[Class 0] Epoch 7/10, Loss=0.2750\n",
      "[Class 0] Epoch 8/10, Loss=0.2723\n",
      "[Class 0] Epoch 9/10, Loss=0.2677\n",
      "[Class 0] Epoch 10/10, Loss=0.2867\n",
      "=== Обучаем модель для класса 1 (one-vs-all) ===\n",
      "[Class 1] Epoch 1/10, Loss=0.4226\n",
      "[Class 1] Epoch 2/10, Loss=0.4044\n",
      "[Class 1] Epoch 3/10, Loss=0.3994\n",
      "[Class 1] Epoch 4/10, Loss=0.3956\n",
      "[Class 1] Epoch 5/10, Loss=0.3921\n",
      "[Class 1] Epoch 6/10, Loss=0.3798\n",
      "[Class 1] Epoch 7/10, Loss=0.3746\n",
      "[Class 1] Epoch 8/10, Loss=0.3693\n",
      "[Class 1] Epoch 9/10, Loss=0.3662\n",
      "[Class 1] Epoch 10/10, Loss=0.3613\n",
      "=== Обучаем модель для класса 2 (one-vs-all) ===\n",
      "[Class 2] Epoch 1/10, Loss=0.5851\n",
      "[Class 2] Epoch 2/10, Loss=0.5610\n",
      "[Class 2] Epoch 3/10, Loss=0.5460\n",
      "[Class 2] Epoch 4/10, Loss=0.5380\n",
      "[Class 2] Epoch 5/10, Loss=0.5452\n",
      "[Class 2] Epoch 6/10, Loss=0.5357\n",
      "[Class 2] Epoch 7/10, Loss=0.5251\n",
      "[Class 2] Epoch 8/10, Loss=0.5252\n",
      "[Class 2] Epoch 9/10, Loss=0.5231\n",
      "[Class 2] Epoch 10/10, Loss=0.5139\n",
      "=== Обучаем модель для класса 3 (one-vs-all) ===\n",
      "[Class 3] Epoch 1/10, Loss=0.4890\n",
      "[Class 3] Epoch 2/10, Loss=0.4608\n",
      "[Class 3] Epoch 3/10, Loss=0.4527\n",
      "[Class 3] Epoch 4/10, Loss=0.4450\n",
      "[Class 3] Epoch 5/10, Loss=0.4418\n",
      "[Class 3] Epoch 6/10, Loss=0.4354\n",
      "[Class 3] Epoch 7/10, Loss=0.4433\n",
      "[Class 3] Epoch 8/10, Loss=0.4300\n",
      "[Class 3] Epoch 9/10, Loss=0.4237\n",
      "[Class 3] Epoch 10/10, Loss=0.4165\n",
      "=== Обучаем модель для класса 4 (one-vs-all) ===\n",
      "[Class 4] Epoch 1/10, Loss=0.4997\n",
      "[Class 4] Epoch 2/10, Loss=0.4737\n",
      "[Class 4] Epoch 3/10, Loss=0.4667\n",
      "[Class 4] Epoch 4/10, Loss=0.4601\n",
      "[Class 4] Epoch 5/10, Loss=0.4528\n",
      "[Class 4] Epoch 6/10, Loss=0.4495\n",
      "[Class 4] Epoch 7/10, Loss=0.4454\n",
      "[Class 4] Epoch 8/10, Loss=0.4420\n",
      "[Class 4] Epoch 9/10, Loss=0.4400\n",
      "[Class 4] Epoch 10/10, Loss=0.4420\n",
      "=== Обучаем модель для класса 5 (one-vs-all) ===\n",
      "[Class 5] Epoch 1/10, Loss=0.3538\n",
      "[Class 5] Epoch 2/10, Loss=0.3455\n",
      "[Class 5] Epoch 3/10, Loss=0.3417\n",
      "[Class 5] Epoch 4/10, Loss=0.3361\n",
      "[Class 5] Epoch 5/10, Loss=0.3374\n",
      "[Class 5] Epoch 6/10, Loss=0.3348\n",
      "[Class 5] Epoch 7/10, Loss=0.3303\n",
      "[Class 5] Epoch 8/10, Loss=0.3282\n",
      "[Class 5] Epoch 9/10, Loss=0.3259\n",
      "[Class 5] Epoch 10/10, Loss=0.3214\n",
      "=== Обучаем модель для класса 6 (one-vs-all) ===\n",
      "[Class 6] Epoch 1/10, Loss=0.3034\n",
      "[Class 6] Epoch 2/10, Loss=0.2848\n",
      "[Class 6] Epoch 3/10, Loss=0.2721\n",
      "[Class 6] Epoch 4/10, Loss=0.2717\n",
      "[Class 6] Epoch 5/10, Loss=0.2838\n",
      "[Class 6] Epoch 6/10, Loss=0.2607\n",
      "[Class 6] Epoch 7/10, Loss=0.2671\n",
      "[Class 6] Epoch 8/10, Loss=0.2581\n",
      "[Class 6] Epoch 9/10, Loss=0.2516\n",
      "[Class 6] Epoch 10/10, Loss=0.2453\n",
      "=== Обучаем модель для класса 7 (one-vs-all) ===\n",
      "[Class 7] Epoch 1/10, Loss=0.3110\n",
      "[Class 7] Epoch 2/10, Loss=0.3004\n",
      "[Class 7] Epoch 3/10, Loss=0.2983\n",
      "[Class 7] Epoch 4/10, Loss=0.2938\n",
      "[Class 7] Epoch 5/10, Loss=0.2866\n",
      "[Class 7] Epoch 6/10, Loss=0.2795\n",
      "[Class 7] Epoch 7/10, Loss=0.2711\n",
      "[Class 7] Epoch 8/10, Loss=0.2605\n",
      "[Class 7] Epoch 9/10, Loss=0.2629\n",
      "[Class 7] Epoch 10/10, Loss=0.2524\n",
      "Все модели успешно обучены.\n"
     ]
    }
   ],
   "source": [
    "def train_one_vs_all_models():\n",
    "    # Параметры\n",
    "    \n",
    "    input_dim = 2\n",
    "    num_classes = 8\n",
    "    hidden_dim = 256\n",
    "    bidirectional = True\n",
    "    batch_size = 128\n",
    "    num_epochs = 10\n",
    "    num_layers = 2\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Загружаем датасет\n",
    "    dataset = SiamDataset(frame)\n",
    "    data_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn_varlen\n",
    "    )\n",
    "    \n",
    "    # Для каждого класса создадим свою модель\n",
    "    models = []\n",
    "    for c in range(num_classes):\n",
    "        print(f\"=== Обучаем модель для класса {c} (one-vs-all) ===\")\n",
    "        \n",
    "        model_c = LSTMSingleClassModel(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers= num_layers\n",
    "        ).to(device)\n",
    "        \n",
    "        # Обучаем\n",
    "        trained_model_c = train_single_class(\n",
    "            model_c, \n",
    "            data_loader,\n",
    "            device=device,\n",
    "            lr=1e-3,\n",
    "            num_epochs=num_epochs,\n",
    "            class_index=c\n",
    "        )\n",
    "        \n",
    "        models.append(trained_model_c)\n",
    "    \n",
    "    print(\"Все модели успешно обучены.\")\n",
    "    return models\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_class_models = train_one_vs_all_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_vs_all(models, x, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    models: список, где models[c] - LSTMSingleClassModel для класса c\n",
    "    x: np.array shape [T, input_dim] (один объект) \n",
    "    \"\"\"\n",
    "    # Превращаем в батч размером 1\n",
    "    T = x.shape[0]\n",
    "    x_tensor = torch.from_numpy(x).unsqueeze(0)  # [1, T, input_dim]\n",
    "    lengths = torch.tensor([T], dtype=torch.long)\n",
    "    \n",
    "    # Будем хранить предсказания [num_classes]\n",
    "    preds = []\n",
    "    \n",
    "    for c, model_c in enumerate(models):\n",
    "        model_c.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model_c(x_tensor.to(device), lengths.to(device))\n",
    "            # logits shape: [1, 1]\n",
    "            prob = torch.sigmoid(logits)[0, 0].item()\n",
    "            pred_label = 1 if prob >= threshold else 0\n",
    "            preds.append(pred_label)\n",
    "    \n",
    "    return preds  # список [0/1] длиной num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 1]\n",
      " [0 1 1 ... 0 1 0]\n",
      " [0 1 1 ... 0 0 0]]\n",
      "[[0. 1. 1. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n",
      "Accuracy по полным классам 0.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision_macro': 0.7452076677316294,\n",
       " 'recall_macro': 0.7566909975669099,\n",
       " 'f1_macro': 0.7509054325955734,\n",
       " 'accuracy': 0.84525,\n",
       " 'hamming_loss': 0.15475}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss, accuracy_score\n",
    "\n",
    "def evaluate_one_vs_all_models(models, data_loader, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Оценивает набор моделей (one-vs-all) по метрикам precision, recall, f1, hamming_loss.\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    models : list of nn.Module\n",
    "        Список моделей, где models[c] - это LSTMSingleClassModel (или любая бинарная модель)\n",
    "        для класса c.\n",
    "    data_loader : DataLoader\n",
    "        Должен возвращать батчи в формате (x_padded, lengths, y_true), где:\n",
    "          - x_padded: [batch_size, max_len, input_dim]\n",
    "          - lengths: [batch_size] (длины последовательностей)\n",
    "          - y_true: [batch_size, num_classes] (истинные метки для всех классов)\n",
    "    device : torch.device\n",
    "        'cuda' или 'cpu'\n",
    "    threshold : float\n",
    "        Порог для перевода предсказанных вероятностей в 0/1.\n",
    "    \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    metrics_dict : dict\n",
    "        Словарь вида {\n",
    "          'precision_macro': ...,\n",
    "          'recall_macro': ...,\n",
    "          'f1_macro': ...,\n",
    "          'hamming_loss': ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Определим количество классов на основе длины списка моделей\n",
    "    num_classes = len(models)\n",
    "    \n",
    "    # Переведём все модели в режим eval\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    with torch.no_grad():\n",
    "        for x_padded, lengths, y_true in data_loader:\n",
    "            # x_padded: [batch_size, max_len, input_dim]\n",
    "            # lengths: [batch_size]\n",
    "            # y_true: [batch_size, num_classes]\n",
    "            x_padded = x_padded.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            # Будем собирать предсказания по всем классам для этого батча\n",
    "            # batch_preds: [batch_size, num_classes]\n",
    "            batch_preds = []\n",
    "            batch_probs = []\n",
    "            for c, model_c in enumerate(models):\n",
    "                # Прогоняем i-й класс\n",
    "                logits = model_c(x_padded, lengths).squeeze(-1)  # [batch_size]\n",
    "                probs = torch.sigmoid(logits)                    # [batch_size]\n",
    "                preds_c = (probs >= threshold).long()            # [batch_size]\n",
    "                # print(logits)\n",
    "                # Расширяем до [batch_size, 1] и добавим в список\n",
    "                batch_preds.append(preds_c.unsqueeze(-1))\n",
    "                batch_probs.append(logits.unsqueeze(-1))\n",
    "            \n",
    "            # Склеим вдоль последней оси => [batch_size, num_classes]\n",
    "            # print(batch_probs)\n",
    "            batch_preds = torch.cat(batch_preds, dim=1)\n",
    "            batch_probs = torch.cat(batch_probs, dim=1)\n",
    "            all_probs.append(batch_probs.cpu().numpy())\n",
    "            all_preds.append(batch_preds.cpu().numpy())\n",
    "            all_labels.append(y_true.numpy())  # y_true уже float на CPU (смотри коллектор)\n",
    "\n",
    "    # Склеиваем все батчи в один массив\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_preds = np.concatenate(all_preds, axis=0)   # [N, num_classes]\n",
    "    all_labels = np.concatenate(all_labels, axis=0) # [N, num_classes]\n",
    "    \n",
    "    # Метрики. Можно менять 'macro'/'micro' и т.д. по вкусу.\n",
    "    prec_macro = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    rec_macro  = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    f1_macro   = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "    accuracy = accuracy_score(all_labels.ravel(), all_preds.ravel())\n",
    "    h_loss     = hamming_loss(all_labels, all_preds)\n",
    "    print(all_preds)\n",
    "    print(all_labels)\n",
    "    with open(\"./preds_labels.txt\", \"w\") as f:\n",
    "        f.write(\"[\" + \",\".join([str(i) for prob in all_probs for i in prob]) + \"]\")\n",
    "        \n",
    "    metrics_dict = {\n",
    "        'precision_macro': prec_macro,\n",
    "        'recall_macro': rec_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        \"accuracy\": accuracy,\n",
    "        'hamming_loss': h_loss\n",
    "    }\n",
    "    print(f\"Accuracy по полным классам {np.sum(np.all(all_preds == all_labels, axis=1))/all_preds.shape[0]}\")\n",
    "    return metrics_dict\n",
    "\n",
    "dataset = SiamDataset(frame_test)\n",
    "data_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn_varlen\n",
    "    )\n",
    "\n",
    "evaluate_one_vs_all_models(loaded_models, data_loader, threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель для класса 0 сохранена в saved_models\\modelv4_withoutGood_class_0.pt\n",
      "Модель для класса 1 сохранена в saved_models\\modelv4_withoutGood_class_1.pt\n",
      "Модель для класса 2 сохранена в saved_models\\modelv4_withoutGood_class_2.pt\n",
      "Модель для класса 3 сохранена в saved_models\\modelv4_withoutGood_class_3.pt\n",
      "Модель для класса 4 сохранена в saved_models\\modelv4_withoutGood_class_4.pt\n",
      "Модель для класса 5 сохранена в saved_models\\modelv4_withoutGood_class_5.pt\n",
      "Модель для класса 6 сохранена в saved_models\\modelv4_withoutGood_class_6.pt\n",
      "Модель для класса 7 сохранена в saved_models\\modelv4_withoutGood_class_7.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Папка для сохранения моделей\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Сохраняем каждую модель\n",
    "for idx, model in enumerate(all_class_models):\n",
    "    # Сохраняем состояние модели\n",
    "    model_path = os.path.join(save_dir, f\"modelv5_withoutGood_class_{idx}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Модель для класса {idx} сохранена в {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель для класса 0 загружена из saved_models\\modelv4_withoutGood_class_0.pt\n",
      "Модель для класса 1 загружена из saved_models\\modelv4_withoutGood_class_1.pt\n",
      "Модель для класса 2 загружена из saved_models\\modelv4_withoutGood_class_2.pt\n",
      "Модель для класса 3 загружена из saved_models\\modelv4_withoutGood_class_3.pt\n",
      "Модель для класса 4 загружена из saved_models\\modelv4_withoutGood_class_4.pt\n",
      "Модель для класса 5 загружена из saved_models\\modelv4_withoutGood_class_5.pt\n",
      "Модель для класса 6 загружена из saved_models\\modelv4_withoutGood_class_6.pt\n",
      "Модель для класса 7 загружена из saved_models\\modelv4_withoutGood_class_7.pt\n"
     ]
    }
   ],
   "source": [
    "loaded_models = []\n",
    "import os\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "input_dim = 2\n",
    "num_classes = 8\n",
    "hidden_dim = 256\n",
    "bidirectional = True\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "num_layers = 2\n",
    "save_dir = \"saved_models\"\n",
    "\n",
    "for idx in range(num_classes):\n",
    "    # Создаём экземпляр модели\n",
    "    model_c = LSTMSingleClassModel(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        bidirectional=bidirectional,\n",
    "        num_layers=num_layers\n",
    "    ).to(device)\n",
    "    \n",
    "    # Загружаем сохранённые веса\n",
    "    model_path = os.path.join(save_dir, f\"modelv4_withoutGood_class_{idx}.pt\")\n",
    "    model_c.load_state_dict(torch.load(model_path))\n",
    "    model_c.eval()  # Переводим модель в режим оценки\n",
    "    \n",
    "    loaded_models.append(model_c)\n",
    "    print(f\"Модель для класса {idx} загружена из {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
