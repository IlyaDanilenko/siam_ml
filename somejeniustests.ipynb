{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "frame = pd.read_csv(\"hq_markup_train.csv\")\n",
    "\n",
    "def get_sample(index, frame):\n",
    "    uuid = frame.iloc[index, 0]\n",
    "    with open(f'data/{uuid}', 'r') as f:\n",
    "        content = f.read()\n",
    "    rows = [line.split('\\t') for line in content.strip().split('\\n')]\n",
    "    data = pd.DataFrame(rows, columns=['time', 'delta_p', 'p_'], dtype=float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiamDataset(Dataset):\n",
    "    def __init__(self, siam_dataset_describe:pd.DataFrame):\n",
    "        super().__init__()\n",
    "        self.siam_dataset_describe = siam_dataset_describe\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.siam_dataset_describe.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = get_sample(idx, self.siam_dataset_describe) #.to_numpy(dtype=np.float64)\n",
    "        # t = x[\"time\"].to_numpy(dtype=np.float64)\n",
    "        x = x[[\"delta_p\", \"p_\"]].to_numpy(dtype=np.float64)\n",
    "        \n",
    "\n",
    "\n",
    "        # 7) Возвращаем (X, Y)\n",
    "        return x, self.siam_dataset_describe.iloc[idx][['Некачественное ГДИС', 'Влияние ствола скважины', 'Радиальный режим', 'Линейный режим', 'Билинейный режим', 'Сферический режим', 'Граница постоянного давления', 'Граница непроницаемый разлом']].to_numpy(dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_batch1(batch):\n",
    "    \"\"\"\n",
    "    collate_fn, который просто возвращает список длиной batch_size=1,\n",
    "    так как иначе нужно продумывать, как совмещать разные временные метки в одном батче.\n",
    "    \"\"\"\n",
    "    # batch - это список из [(t_i, x_i, y_i), (t_j, x_j, y_j), ...]\n",
    "    # При batch_size=1 там будет ровно 1 элемент.\n",
    "    return batch[0]  # вернём (t, x, y) без упаковки в дополнительные измерения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(trained_model.state_dict(), './TransformerV2.pth')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_varlen(batch):\n",
    "    \"\"\"\n",
    "    batch - список из [(x_i, y_i), (x_j, y_j), ...] размера batch_size.\n",
    "      x_i: [T_i, input_dim]\n",
    "      y_i: [num_classes]\n",
    "    \n",
    "    Задача: паддить x_i до [max_len, input_dim] и собрать в один тензор.\n",
    "            Сформировать маску pad_mask для вычисления attention.\n",
    "    \"\"\"\n",
    "    # 1) Определяем максимальную длину в батче\n",
    "    max_len = 1800\n",
    "    input_dim = batch[0][0].shape[1]\n",
    "    # num_classes = batch[0][1].shape[0]\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # 2) Подготовим тензоры для x, y, mask\n",
    "    x_padded = torch.zeros(batch_size, max_len, input_dim, dtype=torch.float32)\n",
    "    pad_mask = torch.ones(batch_size, max_len, dtype=torch.bool)  # True = padded (будем инвертировать при attention)\n",
    "    y_tensor = torch.zeros(batch_size, 8, dtype=torch.float32)\n",
    "    \n",
    "    # 3) Заполним их\n",
    "    for i, (x_i, y_i) in enumerate(batch):\n",
    "        length = min(x_i.shape[0], max_len)\n",
    "        # print(x_i)\n",
    "        # raise Exception(\"STOP\")\n",
    "        x_padded[i, :length, :] = torch.from_numpy(x_i[:length])\n",
    "        pad_mask[i, :length] = False  # False => там реальные данные\n",
    "        y_tensor[i] = torch.from_numpy(y_i)\n",
    "    \n",
    "    # Возвращаем (x_padded, pad_mask, y_tensor)\n",
    "    return x_padded, pad_mask, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Проекция входа в d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # Мульти-хед внимание\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True  # сделаем batch_first=True (поддерживается в новых версиях PyTorch)\n",
    "        )\n",
    "        \n",
    "        # Финальный классификатор (multi-label => просто линейный + BCEWithLogitsLoss в конце)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x_padded, pad_mask):\n",
    "        \"\"\"\n",
    "        x_padded: [batch_size, seq_len, input_dim]\n",
    "        pad_mask: [batch_size, seq_len] (True => позиция паддинга)\n",
    "        \n",
    "        Возвращает logits: [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # 1) Проекция\n",
    "        x_proj = self.embedding(x_padded)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 2) Применяем MultiheadAttention\n",
    "        #    В self-attention Q=K=V = x_proj, но нужно передать их в MultiheadAttention\n",
    "        #    т.к. batch_first=True, форма входа: [batch_size, seq_len, d_model]\n",
    "        #    pad_mask: [batch_size, seq_len], но для MHA часто нужна shape [batch_size, 1, seq_len, seq_len]\n",
    "        #    Однако с batch_first=True с недавних пор PyTorch поддерживает форму [batch_size, seq_len].\n",
    "        \n",
    "        # query, key, value = x_proj (self-attention)\n",
    "        # attn_mask => None, но key_padding_mask => pad_mask\n",
    "        attn_out, attn_weights = self.attention(\n",
    "            x_proj, x_proj, x_proj,\n",
    "            key_padding_mask=pad_mask\n",
    "        )\n",
    "        # attn_out: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 3) Pooling по временной оси, чтобы получить 1 вектор на объект\n",
    "        #    Возьмём mean pooling по тем позициям, которые не паддинг.\n",
    "        #    Для корректного усреднения учтём число реальных токенов.\n",
    "        #    (Можно использовать маску, чтобы не считать паддинг.)\n",
    "        \n",
    "        # Считаем количество реальных (не паддинговых) токенов по каждому объекту\n",
    "        real_lengths = (~pad_mask).sum(dim=1).unsqueeze(-1)  # [batch_size, 1]\n",
    "        \n",
    "        # Суммируем embeddings по seq_len\n",
    "        sum_embeddings = torch.sum(attn_out * (~pad_mask).unsqueeze(-1), dim=1)  \n",
    "        # sum_embeddings: [batch_size, d_model]\n",
    "        #  т.к. (~pad_mask) имеет форму [batch_size, seq_len], мы делаем broadcast по последней оси\n",
    "        \n",
    "        # Теперь усредним\n",
    "        avg_embeddings = sum_embeddings / real_lengths.clamp(min=1e-9)  # [batch_size, d_model]\n",
    "        \n",
    "        # 4) Применяем линейный классификатор\n",
    "        logits = self.classifier(avg_embeddings)  # [batch_size, num_classes]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n",
      "Epoch [1/30] - Loss: 30.6023\n",
      "Epoch [2/30] - Loss: 10.2743\n",
      "Epoch [3/30] - Loss: 4.3890\n",
      "Epoch [4/30] - Loss: 3.3025\n",
      "Epoch [5/30] - Loss: 3.1909\n",
      "Epoch [6/30] - Loss: 2.3690\n",
      "Epoch [7/30] - Loss: 1.9273\n",
      "Epoch [8/30] - Loss: 2.3237\n",
      "Epoch [9/30] - Loss: 2.2423\n",
      "Epoch [10/30] - Loss: 1.8242\n",
      "Epoch [11/30] - Loss: 2.4121\n",
      "Epoch [12/30] - Loss: 1.8252\n",
      "Epoch [13/30] - Loss: 1.7926\n",
      "Epoch [14/30] - Loss: 1.6189\n",
      "Epoch [15/30] - Loss: 1.8774\n",
      "Epoch [16/30] - Loss: 1.5470\n",
      "Epoch [17/30] - Loss: 1.2401\n",
      "Epoch [18/30] - Loss: 2.1239\n",
      "Epoch [19/30] - Loss: 1.4779\n",
      "Epoch [20/30] - Loss: 1.6902\n",
      "Epoch [21/30] - Loss: 1.6867\n",
      "Epoch [22/30] - Loss: 1.3742\n",
      "Epoch [23/30] - Loss: 1.5401\n",
      "Epoch [24/30] - Loss: 1.6030\n",
      "Epoch [25/30] - Loss: 2.4357\n",
      "Epoch [26/30] - Loss: 1.9523\n",
      "Epoch [27/30] - Loss: 1.6392\n",
      "Epoch [28/30] - Loss: 1.3505\n",
      "Epoch [29/30] - Loss: 1.0310\n",
      "Epoch [30/30] - Loss: 1.1334\n",
      "Обучение завершено.\n"
     ]
    }
   ],
   "source": [
    "def train_attention_model_demo():\n",
    "    # Параметры\n",
    "    # N = 50            # Размер датасета (демонстрация)\n",
    "    input_dim = 2    # Размерность входа в момент времени\n",
    "    num_classes = 8   # Кол-во меток (multi-label)\n",
    "    d_model = 20      # Размерность скрытого представления\n",
    "    num_heads = 2     # Число голов внимания\n",
    "    batch_size = 32\n",
    "    num_epochs = 30\n",
    "    lr = 1e-3\n",
    "    \n",
    "    # 1) Создаём датасет и лоадер\n",
    "    dataset = SiamDataset(frame)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_varlen)\n",
    "    \n",
    "    # 2) Модель\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device is {device}\")\n",
    "    model = AttentionClassifier(input_dim, d_model, num_heads, num_classes).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()  # multi-label\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 3) Тренировочный цикл\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x_padded, pad_mask, y in loader:\n",
    "            x_padded = x_padded.to(device)  # [batch_size, seq_len, input_dim]\n",
    "            pad_mask = pad_mask.to(device)  # [batch_size, seq_len] (bool)\n",
    "            y = y.to(device)                # [batch_size, num_classes]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(x_padded, pad_mask)  # [batch_size, num_classes]\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Обучение завершено.\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_attention_model_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rinning at cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "# trained_model.to(device=device)\n",
    "print(f\"rinning at {device}\")\n",
    "dataset = SiamDataset(frame)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn_varlen)\n",
    "all_proba = []\n",
    "all_real_y = []\n",
    "for x_padded, pad_mask, y in loader:\n",
    "    x_padded = x_padded.to(device)  # [batch_size, seq_len, input_dim]\n",
    "    pad_mask = pad_mask.to(device)  # [batch_size, seq_len] (bool)\n",
    "    # y = y.to(device)                # [batch_size, num_classes]\n",
    "    \n",
    "    logits = trained_model(x_padded, pad_mask)  # [batch_size, num_classes]\n",
    "    pred_proba = torch.sigmoid(logits)\n",
    "    all_proba.append(pred_proba)\n",
    "    all_real_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "[tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 1.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 0., 0., 1., 0., 1., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 1., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 1., 0., 1.]), tensor([0., 0., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 1., 0.]), tensor([0., 0., 0., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 1., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 1., 0., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 1.]), tensor([0., 0., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 0., 1., 1., 0.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([1., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([1., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 1., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([1., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 0., 1.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 0., 1., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 1., 0., 0.]), tensor([0., 0., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 1., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 1., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 0., 1., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 0., 1.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([1., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 1., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 0., 1.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 1., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 1., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([1., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([1., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 1., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 0.]), tensor([1., 1., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 1., 0.]), tensor([0., 0., 1., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 0., 0., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 0., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 0., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 1., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 0., 1., 1., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([1., 1., 1., 1., 0., 0., 0., 0.]), tensor([1., 0., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 1., 1., 0., 0., 0.]), tensor([1., 0., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 1., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 0., 0., 1., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 1., 1., 0.]), tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 1., 0.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 1., 1., 0., 0., 0., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([1., 0., 0., 0., 0., 0., 0., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 0., 1.]), tensor([0., 1., 1., 0., 1., 0., 0., 1.]), tensor([0., 1., 1., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 0., 1., 0.]), tensor([0., 1., 0., 0., 0., 1., 0., 0.]), tensor([0., 1., 1., 0., 0., 1., 0., 1.]), tensor([0., 1., 1., 0., 0., 1., 0., 0.])]\n",
      "[tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([1, 0, 0, 0, 0, 0, 0, 1], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 0, 1, 0, 0, 1, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 1, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([1, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([1, 0, 0, 0, 0, 0, 0, 1], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([1, 1, 1, 0, 1, 0, 0, 1], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 1, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 0, 1, 0, 0, 1, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 0, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 1, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32), tensor([0, 1, 1, 0, 0, 0, 0, 0], dtype=torch.int32)]\n"
     ]
    }
   ],
   "source": [
    "# all_real_y_2 = np.array([tensor[0].cpu().tolist() for tensor in all_real_y]).ravel()\n",
    "# pred_proba_2 = [tensor.cpu() for tensor in all_proba]\n",
    "# all_answers_2 = np.array([(pred_proba[0] > 0.5).int().cpu().tolist() for pred_proba in all_proba]).ravel()\n",
    "\n",
    "all_real_y_2 = [tensor[0].cpu() for tensor in all_real_y]\n",
    "pred_proba_2 = [tensor.cpu() for tensor in all_proba]\n",
    "all_answers_2 = [(pred_proba[0] > 0.5).int().cpu() for pred_proba in all_proba]\n",
    "# # all_answers_2 = \n",
    "print(len(all_real_y_2))\n",
    "print(len(all_answers_2))\n",
    "print(all_real_y_2)\n",
    "print(all_answers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4065487054329372, Recall: 0.26189187762238375, F1: 0.21893089918707404, Hamming Loss: 0.24575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss\n",
    "precision = precision_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "recall = recall_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "f1 = f1_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "hamming = hamming_loss(all_real_y_2, all_answers_2)\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}, Hamming Loss: {hamming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.5107\n",
      "Epoch [2/10] - Loss: 0.4391\n",
      "Epoch [3/10] - Loss: 0.4407\n",
      "Epoch [4/10] - Loss: 0.4262\n",
      "Epoch [5/10] - Loss: 0.4432\n",
      "Epoch [6/10] - Loss: 0.4245\n",
      "Epoch [7/10] - Loss: 0.4242\n",
      "Epoch [8/10] - Loss: 0.4032\n",
      "Epoch [9/10] - Loss: 0.3986\n",
      "Epoch [10/10] - Loss: 0.3899\n",
      "Обучение завершено.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def collate_fn_varlen(batch):\n",
    "    \"\"\"\n",
    "    batch: список из [(x_i, y_i), (x_j, y_j), ...] длиной batch_size\n",
    "      - x_i: np.ndarray shape [T_i, input_dim]\n",
    "      - y_i: np.ndarray shape [num_classes]\n",
    "\n",
    "    Задача: \n",
    "      1) Определить max_len в батче\n",
    "      2) Паддить все x_i до [max_len, input_dim]\n",
    "      3) Сохранить длины T_i\n",
    "      4) Объединить y_i в тензор [batch_size, num_classes]\n",
    "    \"\"\"\n",
    "    # Определяем максимальную длину в данном батче\n",
    "    max_len = max(x.shape[0] for x, _ in batch)\n",
    "    batch_size = len(batch)\n",
    "    input_dim = batch[0][0].shape[1]\n",
    "    num_classes = batch[0][1].shape[0]\n",
    "    num_classes = 2\n",
    "    # print(num_classes)\n",
    "    \n",
    "    # Создаём пустой тензор для x\n",
    "    x_padded = torch.zeros(batch_size, max_len, input_dim, dtype=torch.float32)\n",
    "    lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "    y_tensor = torch.zeros(batch_size, num_classes, dtype=torch.float32)\n",
    "    \n",
    "    for i, (x_i, y_i) in enumerate(batch):\n",
    "        T_i = x_i.shape[0]\n",
    "        if y_i[0] == 1:\n",
    "            y_tensor[i] = torch.Tensor([0., 1.])\n",
    "        else:\n",
    "            y_tensor[i] = torch.Tensor([1., 0.])\n",
    "        x_padded[i, :T_i, :] = torch.from_numpy(x_i)\n",
    "        lengths[i] = T_i\n",
    "        # y_tensor[i] = torch.from_numpy(y_i)[0]\n",
    "         \n",
    "    \n",
    "    return x_padded, lengths, y_tensor\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 3. Модель на базе LSTM + классификатор\n",
    "#######################################################\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM, которая принимает последовательность переменной длины с помощью\n",
    "    pack_padded_sequence. Финальное скрытое состояние (или усреднение) идёт\n",
    "    в линейный классификатор для multi-label (BCEWithLogits).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        dir_factor = 2 if bidirectional else 1\n",
    "        self.classifier = nn.Sequential(nn.Linear(hidden_dim * dir_factor, num_classes), nn.Softmax(dim=1))\n",
    "    \n",
    "    def forward(self, x_padded, lengths):\n",
    "        \"\"\"\n",
    "        x_padded: [batch_size, max_len, input_dim]\n",
    "        lengths: [batch_size], длины для pack_padded_sequence\n",
    "        \"\"\"\n",
    "        batch_size, max_len, _ = x_padded.size()\n",
    "        \n",
    "        # Упаковываем (pack) последовательность, чтобы LSTM пропускала \"пустые\" шаги быстрее\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            x_padded, \n",
    "            lengths.cpu(),  # lengths лучше передать на cpu, если они на gpu\n",
    "            batch_first=True, \n",
    "            enforce_sorted=False  # Позволяет не сортировать batch по длине\n",
    "        )\n",
    "        \n",
    "        # Прогоняем через LSTM\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        \n",
    "        # Можно получить все выходные вектора:\n",
    "        # output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # shape output: [batch_size, max_len, hidden_dim * dir_factor]\n",
    "\n",
    "        # Но для классификации обычно берут последнее скрытое состояние\n",
    "        # h_n: [num_layers * dir_factor, batch_size, hidden_dim]\n",
    "        if self.bidirectional:\n",
    "            # Для bidirectional нужно склеить h_n из прямого и обратного прохода\n",
    "            # Обычно берём h_n[-2] и h_n[-1], если num_layers=1\n",
    "            # Или можно взять h_n с последнего слоя: h_n[-2,...], h_n[-1,...]\n",
    "            # Ниже вариант для общего случая num_layers:\n",
    "            h_last_layer = h_n.view(self.num_layers, 2, batch_size, self.hidden_dim)\n",
    "            # h_last_layer[-1]: shape [2, batch_size, hidden_dim]\n",
    "            h_forward = h_last_layer[-1, 0, :, :]\n",
    "            h_backward = h_last_layer[-1, 1, :, :]\n",
    "            h_final = torch.cat([h_forward, h_backward], dim=1)  # [batch_size, 2*hidden_dim]\n",
    "        else:\n",
    "            # Если одна направленность, берём последний слой h_n[-1]\n",
    "            h_final = h_n[-1]  # shape [batch_size, hidden_dim]\n",
    "        \n",
    "        # Линейная классификация\n",
    "        logits = self.classifier(h_final)  # [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# 4. Пример тренировки (multi-label)\n",
    "#######################################################\n",
    "\n",
    "def train_varlen_demo():\n",
    "    # Гиперпараметры для примера\n",
    "    input_dim = 2\n",
    "    hidden_dim = 128\n",
    "    num_classes = 2\n",
    "    batch_size = 16\n",
    "    num_epochs = 10\n",
    "    lr = 1e-3\n",
    "    \n",
    "    # Создаём датасет и лоадер\n",
    "    dataset = SiamDataset(siam_dataset_describe=frame)\n",
    "    loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn_varlen\n",
    "    )\n",
    "    \n",
    "    # Создаём модель\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=num_classes,\n",
    "        num_layers=2,\n",
    "        bidirectional=True  # для примера включим двунаправленную LSTM\n",
    "    ).to(device)\n",
    "    \n",
    "    # Лосс и оптимизатор\n",
    "    criterion = nn.CrossEntropyLoss()  # multi-label\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Тренировочный цикл\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x_padded, lengths, y_true in loader:\n",
    "            x_padded = x_padded.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(x_padded, lengths)  # [batch_size, num_classes]\n",
    "            # print(logits)\n",
    "            # print(y_true)\n",
    "            loss = criterion(logits, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"Обучение завершено.\")\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = train_varlen_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rinning at cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = \"cpu\"\n",
    "# trained_model.to(device=device)\n",
    "print(f\"rinning at {device}\")\n",
    "dataset = SiamDataset(frame)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn_varlen)\n",
    "all_proba = []\n",
    "all_real_y = []\n",
    "for x_padded, lengths, y in loader:\n",
    "    x_padded = x_padded.to(device)  # [batch_size, seq_len, input_dim]\n",
    "    pad_mask = lengths.to(device)  # [batch_size, seq_len] (bool)\n",
    "    # y = y.to(device)                # [batch_size, num_classes]\n",
    "    \n",
    "    logits = trained_model(x_padded, lengths)  # [batch_size, num_classes]\n",
    "    pred_proba = torch.sigmoid(logits)\n",
    "    all_proba.append(pred_proba)\n",
    "    all_real_y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "[tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1)]\n",
      "[tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0)]\n"
     ]
    }
   ],
   "source": [
    "# all_real_y_2 = np.array([tensor[0].cpu().tolist() for tensor in all_real_y]).ravel()\n",
    "# pred_proba_2 = [tensor.cpu() for tensor in all_proba]\n",
    "# all_answers_2 = np.array([(pred_proba[0] > 0.5).int().cpu().tolist() for pred_proba in all_proba]).ravel()\n",
    "\n",
    "\n",
    "all_real_y_2 = [tensor[0].argmax().cpu() for tensor in all_real_y]\n",
    "pred_proba_2 = [tensor.cpu() for tensor in all_proba]\n",
    "all_answers_2 = [(pred_proba[0] > 0.5).int().cpu() for pred_proba in all_proba]\n",
    "all_answers_2 =  [pred_proba.argmax().cpu() for pred_proba in all_proba]\n",
    "print(len(all_real_y_2))\n",
    "print(len(all_answers_2))\n",
    "print(all_real_y_2)\n",
    "print(all_answers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9174914350970689, Recall: 0.8366759612186948, F1: 0.8694968220712618, Hamming Loss: 0.074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, hamming_loss\n",
    "precision = precision_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "recall = recall_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "f1 = f1_score(all_real_y_2, all_answers_2, average='macro')\n",
    "\n",
    "hamming = hamming_loss(all_real_y_2, all_answers_2)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}, Hamming Loss: {hamming}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
